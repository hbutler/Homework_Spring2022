---
output: 
  pdf_document: 
    keep_tex: yes
    includes:
        in_header: preamble_common.tex
---



STAT 640: Homework 7
===================
Due **Wednesday, March 23, 11:59pm MT** on the course Canvas webpage. Please follow the homework guidelines on the syllabus.

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(ggplot2)
library(Matrix)
```

## Name: Hannah Butler
##


## Problem 1

For this question, use the same data and model as Problem 4 on Homework 6. Consider the null hypothesis that there is no difference in chick weight at age 6 between chicks on Diets 1, 2, and 4. (Note: this hypothesis does not involve Diet 3.)

**a.** Provide the form of the linear model for weight as a function of diet for the entire dataset of age 6 chicks. In other words, copy your answer to Problem 4a from Homework 6.

*********************************************************************************

**Answer:** 
$$\bmY = \bmX\bmbeta + \bmepsilon,$$
where
\begin{align*}
\bmY_{49\times 1} &= 
\begin{bmatrix} 
Y_1 \\ 
\vdots \\ 
Y_49 
\end{bmatrix},
& \bmX_{49\times 4} &=
\begin{bmatrix}
\bm1_{19} & \bm0_{19} & \bm0_{19} & \bm0_{19} \\
\bm0_{10} & \bm1_{10} & \bm0_{10} & \bm0_{10} \\
\bm0_{10} & \bm0_{10} & \bm1_{10} & \bm0_{10} \\
\bm0_{10} & \bm0_{10} & \bm0_{10} & \bm1_{10}
\end{bmatrix},
& \bmbeta_{4\times 1} &= 
\begin{bmatrix}
\beta_1 \\
\beta_2 \\
\beta_3 \\
\beta_4
\end{bmatrix},
& \bmepsilon_{49\times 1} &=
\begin{bmatrix}
\epsilon_1 \\
\vdots \\
\epsilon_{49}
\end{bmatrix}
\end{align*}
and since we will be testing a hypothesis on $\bmbeta$, we must make the distributional assumption
\begin{align*}
\bmepsilon \sim N(\bm0, \sigma^2\bmI_{49\times 49})
\end{align*}

```{r}
cw_6 <- ChickWeight %>%
  filter(Time == 6)

# Response
Y <- cw_6$weight

# Design
X <- cbind(as.numeric(cw_6$Diet == 1)
           , as.numeric(cw_6$Diet == 2)
           , as.numeric(cw_6$Diet == 3)
           , as.numeric(cw_6$Diet == 4)
           )
colnames(X) <- c("diet1"
                 , "diet2"
                 , "diet3"
                 , "diet4"
                 )
```

*********************************************************************************


**b.** Find $\bmA$ such that $\bmA\bmbeta = \bm0$ corresponds to this hypothesis.

*********************************************************************************

**Answer:** 

\begin{align*}
\bmA &=
\begin{bmatrix}
1 & -1 & 0 & 0 \\
0 & 1 & 0 & -1 
\end{bmatrix}
\end{align*}

$\bmA\bmbeta$ is testable because each row of $\bmA$ is estimable (ie, each row of $\bmA$ is a linear combination of the rows of $\bmX$).

Note that with $\bmA$, we are testing the null hypothesis $H_0: \beta_1 = \beta_2$ AND $\beta_2 = \beta_4$. If we fail to reject $H_0$, then this is evidence to suggest that there is no difference between the 3 diets being tested. If we do reject $H_0$, then further testing would be required to determine for which diets there is evidence of a difference. This is because there are three situations in which we should reject $H_0$; (1) $\beta_1 \neq \beta_2, \beta_2 = \beta_4$, (2) $\beta_1 = \beta_2, \beta_2 \neq \beta_4$, and (3) $\beta_1 \neq \beta_2, \beta_2 \neq \beta_4$. In the first two cases, we can infer that one diet differs from the other two which do not differ from each other, but in the third situation, it is not clear whether $\beta_1 = \beta_4$ or not and further testing is needed.

```{r}
# Hypothesis: No difference between diets 1, 2, and 4
A <- rbind(c(1, -1, 0, 0)
           , c(1, 0, 0, -1)
           )
```

*********************************************************************************

**c.** Compute $RSS_H - RSS$ using only $\bbh$, $\bmA$, and $\bmX$.

*********************************************************************************

**Answer:** 
By Proposition 5.4 in the notes, *If $H:\bmA\bmbeta = \bm0$ is a testable hypothesis, then*
$$RSS_H - RSS = (\bmA\bbh)^T\left( \bmA(\bmX^T\bmX)^-\bmA^T\right)^{-1}(\bmA\bbh).$$
Where $\bbh = (\bmX^T\bmX)^-\bmX^T\bmY$.
In R:
```{r}
# (X'X)^-
XX_inv <- solve(t(X) %*% X)

# Find beta estimate
bh <- XX_inv %*% (t(X) %*% Y)

# Compute difference in residual SS
RSS_diff <- t(A %*% bh) %*% solve( A %*% XX_inv %*% t(A) ) %*% (A %*% bh)
RSS_diff
```

*********************************************************************************

**d.** Conduct an F-test to test this hypothesis. Provide the test statistic, p-value, and a conclusion statement.

*********************************************************************************

**Answer:** 
Under the assumption of the null hypothesis $\bmA\bmbeta = \bm0$, we know, by Corollary 5.5.1, that the $F$ statistic is computed as 
$$F = \frac{(RSS_H - RSS)/q}{RSS/(n-r)},$$
and is distributed as ${\cal F}(q, n-r, 0)$. Using this, we can test against and make a decision regarding $H_0$. 

```{r}
n <- length(Y)
r <- rankMatrix(X)[1]
q <- rankMatrix(A)[1]

RSS_full <- t(Y - (X %*% bh)) %*% (Y - (X %*% bh))

# F statistic
fstat <- (RSS_diff/q)/(RSS_full/(n-r))
cat(fstat)

# P-value
cat(1 - pf(fstat, q, n-r))
```

With an F statistic = 25.17208 and a $p$-value of $\approx 4.6 \times 10^{-8}$, we reject the null hypothesis that there is no difference between the average effects of diets 1, 2, and 4 on the weight of 6-day old chicks. In other words, there is evidence in the data to suggest that at least one diet has a different effect on average on the weight of 6-day old chicks.

*********************************************************************************

**e.** Check your answer to (c) by fitting a model that corresponds to the null hypothesis and calculating $RSS_H$.
    
*********************************************************************************

**Answer:** 

Under the null hypothesis $H_0: \beta_1 = \beta_2 = \beta_4$, we would set 
\begin{align*}
\bmX_{H_0} &= 
\begin{bmatrix}
\bm1_{19} & \bm0_{19} \\
\bm1_{10} & \bm0_{10} \\
\bm0_{10} & \bm1_{10} \\
\bm1_{10} & \bm0_{10}
\end{bmatrix}
 & \text{ and } &&
\bmbeta &=
\begin{bmatrix}
\beta_{1,2,4} \\
\beta_3
\end{bmatrix}.
\end{align*}

```{r}
# C1 is diets 1, 2, or 4; C2 is diet 3
X_H0 <- cbind(X[,1] + X[,2] + X[,4]
              , X[,3]
              )

# solve for b_124 and b_3
b_H0 <- solve( t(X_H0)%*%X_H0 ) %*% t(X_H0)%*%Y

# Calculate estimated response under H0
Y_H0 <- X_H0 %*% b_H0

# Calculate RSS under H0
RSS_H0 <- t(Y - Y_H0) %*% (Y - Y_H0)

# Check difference
cat(RSS_H0 - RSS_full)
```

*********************************************************************************


    
# Problem 2

Prove Proposition 5.11. That is, under the conditions of that proposition, show that 
\[\frac{RSS_H - RSS}{RSS} = \frac{R^2 - R^2_H}{1 - R^2}\]

*********************************************************************************

**Answer:** *(See Appendix for statement of Proposition 5.11)*

\begin{align*}
\frac{RSS_H - RSS}{RSS} &= \frac{\bmY^T(\bmP_{\bmX} - \bmP_{\bmX_H})\bmY}{\bmY^T(\bmI - \bmP_{\bmX})\bmY} \\
&= \frac{\bmY^T(\bmP_{\bmX} - \bmJ_n)\bmY - \bmY^T(\bmP_{\bmX_H} - \bmJ_n)\bmY}{\bmY^T(\bmI - \bmJ_n)\bmY - \bmY^T(\bmP_{\bmX} - \bmJ_n)\bmY} \\
&= \frac{RSS - RSS_H}{1 - \bmY^T(\bmP_{\bmX} - \bmJ_n)\bmY/\bmY^T(\bmI - \bmJ_n)\bmY}
\end{align*}

*********************************************************************************



# Problem 3

Consider the two regression lines
\[Y_{ki} = \beta_kx_i + \epsilon_{ki}\]
for $k=1, 2$ and $i=1, \dots, n$. Assume uncorrelated, homoscedastic errors.
Find the F-statistic for testing $H : \beta_1 = \beta_2$.

*********************************************************************************

**Answer:** 
This is a paired test. Consider the model
$$
\bmY_{1} - \bmY_{2} = (\beta_1 - \beta_2)\bmx + (\bmepsilon_{1} - \bm\epsilon_{2}).
$$
Let $\alpha = \beta_1 - \beta_2$ and $\bmzeta = \bmepsilon_{1} - \bmepsilon_{2}.$
Then we have $\bmzeta = \bmepsilon_1 - \bmepsilon_2 \sim N(0, 2\sigma^2\bmI)$.

Now the null hypothesis can be reframed as $H_0: \alpha = 0$ and for an $F$ test we can find $RSS_H - RSS$ as
$$
RSS_H - RSS = (\hat{\alpha} - 0 )^T(\bmx^T\bmx)^{-1}(\hat{\alpha} - 0) = \hat{\alpha}^2(\bmx^T\bmx)
$$
and $RSS$ as
$$
RSS = ((\bmY_1 - \bmY_2) - \hat{\bmY})^T((\bmY_1 - \bmY_2) - \hat{\bmY}).
$$
where $\hat{\bmY} = E[\alpha\bmx + \bmzeta]$.

Since we are testing the value of 1 parameter, $q=1$, and since we have only one predictor, $r = \rank(X) = 1$. So
$$F = \frac{\hat{\alpha}^2(\bmx^T\bmx)}{((\bmY_1 - \bmY_2) - \hat{\bmY})^T((\bmY_1 - \bmY_2) - \hat{\bmY})/(n-1)}.$$
and under the assumption of $H_0: \alpha = 0$, $F \sim {\cal F}(1, n-1, 0)$.

*********************************************************************************


# Problem 4
Consider the linear model $\bmY = \bmX\bmbeta + \bmepsilon$ with $\bmX \in \mathbb{R}^{n \times p}$ and $\rank(\bmX) = r$. Let $\bmA\bmbeta = \bm0$ be a testable hypothesis with $\bmA \in \mathbb{R}^{q \times p}$ and $q < r$. Prove that if $\rank(\bmA) = q$, then $\rank(\bmA(\XtX)^{-}\bmA^\mT) = q$. (Hint: recall that $\rank(\bmB\bmB^\mT) = \rank(\bmB)$.)


*********************************************************************************

**Answer:** 
We can first use the property $\rank(\bmA\bmB) \leq \min(\rank(\bmA), \rank(\bmB))$ to show that
\begin{align*}
\rank(\bmA(\bmX^T\bmX)^- \bmA^T) \le \min(\rank(\bmA), \rank((\bmX^T\bmX)^- \bmA^T)),
\end{align*}
So $\rank(\bmA(\bmX^T\bmX)^- \bmA^T) \le q$. Then by Definition 5.1, we can write $\bmA = \bmM\bmX$. So we have
\begin{align*}
\rank(\bmA(\bmX^T\bmX)^- \bmA^T) &= \rank(\bmM\bmX (\bmX^T\bmX)^- \bmX^T \bmM^T) \\
&= \rank(\bmM \bmP_{\bmX} \bmM^T) \\
&= \rank(\bmM \bmP_{\bmX} \bmP_{\bmX}^T\bmM^T) \\
&= \rank(\bmM\bmP_{\bmX}(\bmM\bmP_{\bmX})^T) \\
&= \rank(\bmM\bmP_{\bmX}).
\end{align*}
Then, again using the property $\rank(\bmA\bmB) \leq \min(\rank(\bmA), \rank(\bmB))$, 
\begin{align*}
\rank(\bmA) &= \rank(\bmM\bmX) \\
&= \rank(\bmM\bmP_{\bmX}\bmX) \\
&\leq \min(\rank(\bmM\bmP_{\bmX}), \rank(\bmX))
\end{align*}

So that $\rank(\bmM\bmP_{\bmX}) = \rank(\bmA(\bmX^T\bmX)^- \bmA^T) \ge q$. Therefore, $\rank(\bmA(\bmX^T\bmX)^- \bmA^T) = q$.

*********************************************************************************




# Problem 5

Consider the linear model
\begin{align*}
Y_1 &= \theta_1 + \theta_2 + \epsilon_1\\
Y_2 &= 2\theta_2 + \epsilon_2\\
Y_3 &= -\theta_1 + \theta_2 + \epsilon_3
\end{align*}
where $\E[\bmepsilon] = \bm0$ and $\Var(\bmepsilon) = \sigma^2\bmI$.



**a.** Show that $H : \theta_1 = 2\theta_2$ is a testable hypothesis.

*********************************************************************************

**Answer:** 
$H : \theta_1 - 2\theta_2$

the vector $\bmg^T = \begin{bmatrix} 1 & -2 \end{bmatrix}$ is estimable (and hence $\bma = \bmg^T$ is testable) because
$$
\bmg^T = \begin{bmatrix} 0 & -1/2 & -1 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 0 & 2 \\ -1 & 1\end{bmatrix} = \begin{bmatrix} 1 & -2 \end{bmatrix},
$$
where $\begin{bmatrix} 1 & 1 \\ 0 & 2 \\ -1 & 1\end{bmatrix}$ is the design matrix $\bmX$.

*********************************************************************************
    
    
**b.** Derive the form of the F-statistic for testing $H$.


*********************************************************************************

**Answer:** 

First finding the components of the $F$-statistic, we have
$$
(\bmX^T\bmX)^- = \begin{bmatrix} 2 & 0 \\ 0 & 6 \end{bmatrix}^{-1} = \begin{bmatrix} 1/2 & 0 \\ 0 & 1/6 \end{bmatrix}.
$$
Then 
$$
\left(\bmA(\bmX^T\bmX)^- \bmA^T \right)^{-1} = \begin{bmatrix} 1 & -2 \end{bmatrix} \begin{bmatrix} 1/2 & 0 \\ 0 & 1/6 \end{bmatrix} \begin{bmatrix} 1 \\ -2 \end{bmatrix} = \frac{6}{7}
$$
Computing $\bbh$ next, we have
$$
\bbh = (\bmX^T\bmX)^-\bmX^T\bmY = \begin{bmatrix} 1/2 & 0 \\ 0 & 1/6 \end{bmatrix} 
\begin{bmatrix} 1 & 0 & -1 \\ 1& 2 & 1\end{bmatrix}
\begin{bmatrix} Y_1 \\ Y_2 \\ Y_3\end{bmatrix}
= \begin{bmatrix} 1/2 & 0 \\ 0 & 1/6 \end{bmatrix} 
\begin{bmatrix} Y_1 - Y_3 \\ Y_1 + 2Y_2 + Y_3 \end{bmatrix}
= \begin{bmatrix} \frac{1}{2}(Y_1 - Y_3) \\ \frac{1}{6}(Y_1 + 2Y_2 + Y_3) \end{bmatrix}.
$$
So
$$
\bma\bbh = \begin{bmatrix} 1 & -2 \end{bmatrix}\begin{bmatrix} \frac{1}{2}(Y_1 - Y_3) \\ \frac{1}{6}(Y_1 + 2Y_2 + Y_3) \end{bmatrix} 
= \frac{1}{2}(Y_1 - Y_3) - \frac{1}{3}(Y_1 + 2Y_2 + Y_3)
= \frac{1}{6}(Y_1 - 4Y_2 - 5Y_3).
$$
We can now compute $RSS_H - RSS$:
$$
RSS_H - RSS = \frac{6}{7}\cdot\frac{1}{36}(Y_1 - 4Y_2 - 5Y_3)^2 = \frac{1}{42} (Y_1 - 4Y_2 - 5Y_3)^2
$$
To compute $RSS$, we need to find $\bmP_{\bmX}$:
\begin{align*}
\bmP_{\bmX} &= \bmX(\bmX^T\bmX)^-\bmX \\
&= \frac{1}{3} 
\begin{bmatrix} 
2 & 1 & -1 \\
1 & 2 & 1 \\
-1 & 1 & 2
\end{bmatrix},
\end{align*}
Then
\begin{align*}
RSS &= \bmY^T(\bmI - \bmP_{\bmX})\bmY \\
&= \frac{1}{3}
\begin{bmatrix} Y_1 & Y_2 & Y_3 \end{bmatrix}
\begin{bmatrix}
1 & -1 & 1 \\
-1 & 1 & -1 \\
1 & -1 & 1
\end{bmatrix}
\begin{bmatrix} Y_1 \\ Y_2 \\ Y_3 \end{bmatrix} \\
&= \frac{1}{3}\left( Y_1^2 + Y_2^2 + Y_3^2 -2(Y_1Y_2 - Y_1Y_3 + Y_2Y_3) \right)
\end{align*}
Then we get
$$
F = \frac{\frac{1}{42} (Y_1 - 4Y_2 - 5Y_3)^2/1}{\frac{1}{3}\left( Y_1^2 + Y_2^2 + Y_3^2 -2(Y_1Y_2 - Y_1Y_3 + Y_2Y_3) \right)/(3-2)}
$$

*********************************************************************************
       
    
**c.** If we assume the errors are normally distributed and the null hypothesis is true, what is the distribution of $F$?


*********************************************************************************

**Answer:** 
By Corollary 5.5.1, $F \sim {\cal F}_{1,1,0}$ under the assumption that $H_0$ is true.

*********************************************************************************
       
\newpage

# Appendix

## List of R packages used:

***

`tidyverse`

`ggplot2`

`Matrix`

## Relevant Definitions

***

**Definition 5.1.** The hypothesis $H: \bmA\bmbeta = \bm0$ is **testable** if $\bma_i^T\bmbeta$ is an estimable function, for each row $\bma_i^T$ of $\bmA$.

**Definition 5.2.** The **residual sum of squares** for a model is $RSS = \sum(Y_i-\hat{Y})^2 = (\bmY - \hat{\bmY})^T(\bmY - \hat{\bmY})$.

**Definition 5.4.** The **sample multiple correlation coefficient** is the correlation between the pairs $(Y_i,\hat{Y_i})$. That is:
$$
R = \frac{\sum_i (Y_i - \bar{Y})(\hat{Y_i} - \bar{\hat{Y_i}})}{\sqrt{\sum_i (Y_i - \bar{Y})^2 \sum_i (\hat{Y_i} - \bar{\hat{Y_i}})^2}}
$$

**Definition 5.5.** The square of the sample multiple correlation coefficient is called the **coefficient of determination**.

## Relevant Propositions

***

**Proposition 5.3** *Consider the two models $\bmY = \bmX_1\bmbeta_1 + \bmepsilon_1$ and $\bmY = \bmX_2\bmbeta_2 + \bmepsilon_1$. The difference in RSS between these models is*
$$
RSS_1 - RSS_2 = \bmY^T(\bmP_{\bmX_2} - \bmP_{bmX_1})\bmY
$$

**Proposition 5.4** *If $H: \bmA\bmbeta = \bm0$ is a testable hypothesis, then*
$$RSS_H - RSS = (\bmA\bbh)^T\left(\bmA(\bmX^T\bmX)^- \bmA^T\right)^{-1}(\bmA\bbh).$$

**Proposition 5.5** *If $\bmY \sim N(\bmmu, \sigma^2\bmI)$ with $\bmmu = \bmX\bmbeta$ and $H: \bmA\bmbeta = \bm0$ is a testable hypothesis with $\rank(\bmA) = q$, then:*
$$F = \frac{(RSS_H - RSS)/q}{RSS/(n-r)} \sim {\cal F}\left( q, n-r, \frac{1}{2\sigma^2}\bmmu^T(\bmP_{\bmX} - \bmP_{\cal W})\bmmu \right),$$
*where ${\cal W} = {\cal N}(\bmM)\cap {\cal C}(\bmX)$ and $\bmM$ is a matrix such that $\bmA = \bmM\bmX$.*

**Proposition 5.11** *In the linear model $\bmY = \bmX\bmbeta + \bmepsilon$, assume $\bmx_1 = \bm1$. Then the test statistic for a hypothesis of the form $H: \begin{bmatrix} \bm0 & \bmA' \end{bmatrix} \bmbeta = \bm0$ (that is, a hypothesis that does not involve the intercept $\beta_1$) can be written:*
$$F = \frac{R^2 - R^2_H}{1 - R^2} \frac{n-p}{q},$$
*where $R^2$ and $R^2_H$ are the coefficients of determination for the full and reduced model, respectively.*

## Relevant Corollaries

***

**Corollary 5.5.1** *If $\bmY sim N(\bmX\bmbeta, \sigma^2\bmI)$ and $H: \bmA\bmbeta = \bm0$ is a testable hypothesis with $\rank(\bmA) = q$, then \textbf{when $H$ is true},*
$$F = \frac{(RSS_H - RSS)/q}{RSS/(n-r)} \sim {\cal F}(q, n-r, 0).$$
