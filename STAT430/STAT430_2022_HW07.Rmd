---
output:
  pdf_document: 
    extra_dependencies: ["bbm", "dsfont"]
  word_document: default
  html_document: default
---

\newcommand{\given}{\,|\,}
\newcommand{\trans}{^\text{T}}
\newcommand{\frechet}{Fr\'echet }
\newcommand{\matern}{Mat\'ern }
\newcommand{\ind}{\mathds{1}}
\newcommand{\iid}{\stackrel{\text{iid}}{\sim}}
\newcommand{\indep}{\stackrel{\text{indep}}{\sim}}


STAT430 Homework #7: Due Friday, April 15, 2022. 
========================================================

#### Name: \textcolor{red}{KEY}

*********************************************************************************

Submit this homework as a pdf file to Canvas. 

## Question 1

Let $Y_1,\ldots,Y_n$ be i.i.d. Uniform$(0,\theta)$.  Show that $Y_{(n)} = \max(Y_1,\ldots,Y_n)$ is sufficient for $\theta$.  Hint:  The Uniform$(0,\theta)$ density can be written $f(y) = \theta^{-1} \cdot {\ind}_{\{0 < y < \theta\}}$, where ${\ind}_{\{A\}}$ is an indicator function which equals 1 if $A$ is true and equals 0 otherwise. 


*********************************************************************************

**Answer:**

\textcolor{red}{
The likelihood function is
\begin{align*}
{\cal L}(\theta \mid \boldsymbol{y}) &= \prod_{i=1}^n \frac{1}{\theta}{\ind}_{\{0 < y_i < \theta \}} 
= \frac{1}{\theta^n} {\ind}_{\{ y_{(n)} < \theta \}} {\ind}_{\{y_{(1)} > 0\}} 
\end{align*}
We see that ${\cal L}(\theta \mid \boldsymbol{y})$ can be written as a product of $g(\theta, Y_{(n)}) = \frac{1}{\theta^n} {\ind}_{\{ y_{(n)} < \theta \}}$ and $h(\boldsymbol{y}) = {\ind}_{\{y_{(1)} > 0\}}$, So by the factorization theorem, $Y_{(n)}$ is a sufficient statistic for $\theta$.
}


*********************************************************************************
## Question 2
Let $Y_1,\ldots,Y_n$ be i.i.d. random variables with density
$$ f(y) = \frac{\alpha y^{\alpha-1}}{\beta^\alpha} \hspace{.5in} 0 < y< \beta.$$
If $\beta$ is known, find a one-dimensional sufficient statistic for $\alpha$.


*********************************************************************************

**Answer:**

\textcolor{red}{
The likelihood function is 
\begin{align*}
{\cal L}(\alpha, \beta \mid \boldsymbol{y_i}) &= \prod_{i=1}^{\infty}\frac{\alpha y^{\alpha-1}}{\beta^\alpha}{\ind}_{\{0 < y_i < \beta \}}
= \left( \frac{\alpha}{\beta^{\alpha}} \right)^{n} \left( \prod_{i=1}^n y_{i} \right)^{\alpha - 1} {\ind}_{\{y_{(1)} > 0 \}}{\ind}_{\{y_{(n)} < \beta\}}.
\end{align*}
which can be factored into a product of $g(\alpha, T(\boldsymbol{y})) = \left( \frac{\alpha}{\beta^{\alpha}} \right)^{n} \left( \prod_{i=1}^n y_{i} \right)^{\alpha - 1}$ and $h(\boldsymbol{y}) = {\ind}_{\{y_{(1)} > 0 \}}{\ind}_{\{y_{(n)} < \beta\}}$, so $T(\boldsymbol{Y}) = \prod_{i=1}^n Y_i$ is a sufficient statistic for $\alpha$.
}


*********************************************************************************

## Question 3

Let $X_1$,...$X_n$ be an observation from the pdf
$$P(X=x ; \theta) = \left(\frac{\theta}{2}  \right)^{|x|} (1-\theta)^{1-|x|}, \hspace{.5in} x = -1, 0,1 \hspace{.5in} 0 \le \theta \le 1.$$

a) Show $T_1 = T(X_1)$ is an unbiased estimator of $\theta$, where $T(X)$ is defined 
\begin{displaymath}
   T(X) = \left\{
     \begin{array}{lr}
       2 & \rm{if \;} x=1\\
       0 & \rm{otherwise}
     \end{array}
   \right.
\end{displaymath} 

Show $T_n = \frac{1}{n}\sum_{i=1}^n T(X_i)$ is unbiased for $\theta$.

*********************************************************************************

**Answer:**

\textcolor{red}{
\begin{align*}
E(T_1) &= T_1(-1) P(X_1 = -1) + T_1(0) P(X_1 = 0) + T_1(1)P(X_1 = 1) = 2\left( \frac{\theta}{2} \right) = \theta
\end{align*}
Since $ET_1 = \theta$, $T_1$ is an unbiased estimator for $\theta$. Similarly, we can show that $T_n$ is an unbiased estimator of $\theta$
\begin{align*}
ET_n &= \frac{1}{n}\sum_{i=1}^nET(X_i) = \frac{n\theta}{n} = \theta
\end{align*}
Since $ET_n = \theta$, $T_n$ is an unbiased estimator for $\theta$.
}


*********************************************************************************

b) Find the MVUE of $\theta$ and show this estimator is better than $T_n$. 


*********************************************************************************

\newpage

**Answer:**

\textcolor{red}{
This was fun to figure out, but definitely a bit tricky. The UMVUE for $\theta$ can be found by taking the expectation of an unbiased estimator of $\theta$ conditioned on a sufficient (and complete) statistic. A sufficient statistic for $\theta$ is $S = \sum_{i=1}^n |X_i|$. I am just going to assume it is complete as well, so the UMVUE for $\theta$ is 
\begin{align*}
E(T_n \mid S = s) &=
\frac{1}{n}\sum_{i=1}^n E(T(X_i) | S = s)
\end{align*}
The expectation $E(T(X_i)|S)$ is
\begin{align*}
E(T(X_i)|S=s) &= T(-1)P(X_i = -1 \mid S = s) + T(0)P(X_i = 0 \mid S = s) + T(1)P(X_i = 1 \mid S = s) \\
&= 2P(X_i = 1 \mid S = s) \\
&=  \frac{2P(X_i = 1, S = s)}{P(S = s)} \\
&=  \frac{2P(X_i = 1, \sum_{j\neq i}|X_j| = s - 1)}{P(S = s)} \\
&=  \frac{2P(X_i = 1)P(\sum_{j\neq i}|X_j| = s - 1)}{P(S = s)} \\
&=  \frac{2(\theta/2) \begin{pmatrix} n-1 \\ s-1 \end{pmatrix} \theta^{s-1}(1-\theta)^{n-s} }{ \begin{pmatrix} n \\ s \end{pmatrix} \theta^s(1-\theta)^{n-s}} \\
&= \frac{\begin{pmatrix} n-1 \\ s-1 \end{pmatrix}}{\begin{pmatrix} n \\ s \end{pmatrix}} 
= \frac{(n-1)!}{(s-1)!(n-s)!} \frac{s!(n-s)!}{n!} 
= \frac{s}{n}
\end{align*}
Try figuring my reasoning out on that. \\
\\
So the expectation $E(T_n|S) = \frac{S}{n} = \frac{1}{n}\sum_{i=1}^n |X_i|$ is the UMVUE for $\theta$. To show that this is a "better" estimator that $T_n$, we have to show that the variance of the UMVUE is smaller than the variance of $T_n$. The variance of $T_n$ is
\begin{align*}
VarT_n &= \frac{1}{n^2}\sum_{i=1}^n VarT(X_i) \\
&= \frac{1}{n^2} \sum_{i=1}^n (ET(X_i)^2 - (ET(X_i))^2) \\
&= \frac{n(4(\theta/2) - (\theta)^2)}{n^2} \\
&= \frac{\theta(2 - \theta)}{n}
\end{align*}
and the variance of $\frac{1}{n}S$ is
\begin{align*}
Var\frac{1}{n}S &= \frac{1}{n^2}\sum_{i=1}^n Var|X_i| \\
&= \frac{1}{n^2}\sum_{i=1}^n (E|X_i|^2 - (E|X_i|)^2) \\
&= \frac{n(\theta/2 + \theta/2 - (\theta/2 + \theta/2)^2)}{n^2} \\
&= \frac{\theta(1- \theta)}{n}
\end{align*}
and it should be easy to see that $\frac{\theta(2 - \theta)}{n} > \frac{\theta(1-\theta)}{n}$, so $S/n$ is better than $T_n$, as it should be.
}


*********************************************************************************

## Question 4
Let $X_1$,...,$X_n$ be i.i.d. Geometric($p$).

a) Find the MoM estimator of $p$.

b) Prove that your estimator in part (a) is consistent for $p$.


*********************************************************************************

**Answer:**



*********************************************************************************

## Question 5
Suppose $Y_1,...,Y_n$ are i.i.d. according to density
$$f(y) = e^{-(y-\theta)}, \hspace{.5in} y \ge \theta, \hspace{.5in} \theta > 0.$$

a) Find the MoM estimator of $\theta$.

b) Is the MoM estimator of $\theta$ unbiased?  If no, compute the bias.

c) Find the variance of the MoM estimator of $\theta$.

d) Find a sufficient statistic for $\theta$.

e) Find the MVUE of $\theta$.

f) Compare the mean squared error of the MoM estimator and the MVUE.  Which one has the smallest MSE?

g) Find the MoM estimator of $\left(log(\theta)\right)^{1/4}$.

*********************************************************************************

**Answer:**



*********************************************************************************


## Question 6
Let $X_1$,...,$X_n$ be i.i.d. Uniform($-\theta$,$\theta$).  

a) Find the MoM estimator of $\theta$.

b) Find a one-dimensional sufficient statistic for $\theta$.

c) Is the MoM estimator you found in part (a) the MVUE?  Explain.

*********************************************************************************

**Answer:**



*********************************************************************************




## Question 7

For each of the following, state whether the statement is TRUE or FALSE.  (\textit{Note: Although I do not ask you to explain WHY each statement is true/false, for an exam you should understand the material well enough that you can WHY.})

1) Suppose $Y_1$,..,$Y_n$ are i.i.d. from distribution with parameter $\theta$, which is a function of $\mu = E[Y_i]$, and  $V[Y_i]<\infty$.  The MoM estimator of $\theta^{-2/5}$ is a consistent estimator.

2) Suppose $X$ is a random variable and $g$ is a function. Then $E[g(X)] = g(E[X])$.

3) The MVUE and MLE are always functions of a minimal sufficient statistic.

4) MoM estimators are always unbiased.

5) MLEs are always unbiased.

6) MVUEs are always unbiased.


*********************************************************************************

**Answer:**



*********************************************************************************

 
